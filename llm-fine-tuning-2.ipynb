{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Load JSON file\nfile_path = \"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    data = json.load(file)\n\n# Extract relevant information\nrecords = []\nfor topic in data[\"data\"]:\n    title = topic[\"title\"]\n    for paragraph in topic[\"paragraphs\"]:\n        context = paragraph[\"context\"]\n        for qas in paragraph[\"qas\"]:\n            question = qas[\"question\"]\n            question_id = qas[\"id\"]\n            answers = [ans[\"text\"] for ans in qas[\"answers\"]]  # Collect all possible answers\n            records.append([title, context, question, question_id, answers])\n\n# Convert to Pandas DataFrame\ndf = pd.DataFrame(records, columns=[\"Title\", \"Context\", \"Question\", \"Question_ID\", \"Answers\"])\n\n# Display DataFrame\nprint(df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:51.811979Z","iopub.execute_input":"2025-02-15T17:31:51.812216Z","iopub.status.idle":"2025-02-15T17:31:53.907830Z","shell.execute_reply.started":"2025-02-15T17:31:51.812185Z","shell.execute_reply":"2025-02-15T17:31:53.906880Z"}},"outputs":[{"name":"stdout","text":"                      Title  \\\n0  University_of_Notre_Dame   \n1  University_of_Notre_Dame   \n2  University_of_Notre_Dame   \n3  University_of_Notre_Dame   \n4  University_of_Notre_Dame   \n\n                                             Context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            Question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                Question_ID                                    Answers  \n0  5733be284776f41900661182               [Saint Bernadette Soubirous]  \n1  5733be284776f4190066117f                [a copper statue of Christ]  \n2  5733be284776f41900661180                        [the Main Building]  \n3  5733be284776f41900661181  [a Marian place of prayer and reflection]  \n4  5733be284776f4190066117e       [a golden statue of the Virgin Mary]  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhuggingface_value = user_secrets.get_secret(\"huggingface\")\nwandb_value = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:53.909115Z","iopub.execute_input":"2025-02-15T17:31:53.909533Z","iopub.status.idle":"2025-02-15T17:31:54.156050Z","shell.execute_reply.started":"2025-02-15T17:31:53.909508Z","shell.execute_reply":"2025-02-15T17:31:54.155193Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=huggingface_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:54.157760Z","iopub.execute_input":"2025-02-15T17:31:54.158078Z","iopub.status.idle":"2025-02-15T17:31:54.814749Z","shell.execute_reply.started":"2025-02-15T17:31:54.158044Z","shell.execute_reply":"2025-02-15T17:31:54.813867Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nimport torch\n#TinyLlama/TinyLlama_v1.1\n#meta-llama/Llama-3.2-3B\n# Load the model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:31:54.816088Z","iopub.execute_input":"2025-02-15T17:31:54.816391Z","iopub.status.idle":"2025-02-15T17:35:01.793322Z","shell.execute_reply.started":"2025-02-15T17:31:54.816361Z","shell.execute_reply":"2025-02-15T17:35:01.792494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333c03308c9e4a26b018680d5ed828e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a7e28a29bf4b299fa0a920969f6a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262d94cefe5c496e8be7de38d9839b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0456a17645e42fa832e5e54b390aa9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f25fc9d5668a4b498f782ed032f86c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d06d4cae014251bb1f4800517b3aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6285f2bd2eac41d2a0406e5f489f07c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94beb577a0a24d96adeacd831287b1d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc81289d21cd49c78e09242f5ec1138b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b6db29342054e149d5eb2aa5cc1f331"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from datasets import Dataset , DatasetDict\nmax_length = 256\ndef tokenize_function(examples):\n    text = [f\"Context: {inp} \\nQuestions:\" for inp in examples[\"Context\"]]\n    \n    examples[\"input_ids\"] = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors = \"pt\").input_ids\n    examples[\"labels\"] = tokenizer(examples[\"Question\"], truncation=True, padding=\"max_length\", max_length=max_length, return_tensors = \"pt\").input_ids\n    \n    return examples\n    \ndata = Dataset.from_pandas(df)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenized_dataset = data.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns(['Title', 'Context', 'Question', 'Question_ID', 'Answers'])\ntokenized_dataset = tokenized_dataset.filter(lambda example,index:index%20==0,with_indices=True)\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:35:01.794232Z","iopub.execute_input":"2025-02-15T17:35:01.794901Z","iopub.status.idle":"2025-02-15T17:36:00.332029Z","shell.execute_reply.started":"2025-02-15T17:35:01.794866Z","shell.execute_reply":"2025-02-15T17:36:00.331400Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"226fc7a0699f4185bfa33bfdb471ea36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c826bf08c964b47a720454c7fd101a0"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha = 32,\n    lora_dropout=0.05,\n    bias='none',\n    task_type = TaskType.SEQ_2_SEQ_LM\n)\npeft_model = get_peft_model(model,peft_config=lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:36:00.332815Z","iopub.execute_input":"2025-02-15T17:36:00.333128Z","iopub.status.idle":"2025-02-15T17:36:00.509041Z","shell.execute_reply.started":"2025-02-15T17:36:00.333090Z","shell.execute_reply":"2025-02-15T17:36:00.508017Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# from datasets import Dataset , DatasetDict\n# split_dataset = DatasetDict({\n#     \"train\": split_dataset[\"train\"].with_format(\"torch\"),  \n#     \"test\": split_dataset[\"test\"].with_format(\"torch\")\n# })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:36:00.510056Z","iopub.execute_input":"2025-02-15T17:36:00.510350Z","iopub.status.idle":"2025-02-15T17:36:00.513650Z","shell.execute_reply.started":"2025-02-15T17:36:00.510313Z","shell.execute_reply":"2025-02-15T17:36:00.512810Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments , Trainer\ntraining_args = TrainingArguments(\n                    output_dir=\" ./llama_3.2_3b_fine-tuned\",\n                    learning_rate = 1e-5,\n                    num_train_epochs = 5,\n                    weight_decay = 0.01,\n                    # per_device_train_batch_size=2,\n                    # per_device_eval_batch_size=2,\n                    # gradient_accumulation_steps=4,\n                    auto_find_batch_size = True,\n                    evaluation_strategy  = 'epoch',\n                    save_strategy=\"epoch\",  # Save checkpoints at each epoch\n                    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n                    load_best_model_at_end=True,  # Load best model after training\n                    fp16=True)\n\ntrainer = Trainer(\n    model = peft_model,\n    # tokenizer = tokenizer,\n    args = training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:36:00.515994Z","iopub.execute_input":"2025-02-15T17:36:00.516190Z","iopub.status.idle":"2025-02-15T17:36:02.347458Z","shell.execute_reply.started":"2025-02-15T17:36:00.516172Z","shell.execute_reply":"2025-02-15T17:36:02.346563Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=wandb_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:36:02.348573Z","iopub.execute_input":"2025-02-15T17:36:02.348885Z","iopub.status.idle":"2025-02-15T17:36:08.372491Z","shell.execute_reply.started":"2025-02-15T17:36:02.348851Z","shell.execute_reply":"2025-02-15T17:36:08.371652Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbahaa_beshoy\u001b[0m (\u001b[33mbahaa_beshoy-helwan-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:36:08.373474Z","iopub.execute_input":"2025-02-15T17:36:08.374190Z","iopub.status.idle":"2025-02-15T18:59:21.513148Z","shell.execute_reply.started":"2025-02-15T17:36:08.374164Z","shell.execute_reply":"2025-02-15T18:59:21.512210Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250215_173609-tv8i7ur4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/tv8i7ur4' target=\"_blank\"> ./llama_3.2_3b_fine-tuned</a></strong> to <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface' target=\"_blank\">https://wandb.ai/bahaa_beshoy-helwan-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/tv8i7ur4' target=\"_blank\">https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/tv8i7ur4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4380' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4380/4380 1:23:01, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.367100</td>\n      <td>0.387965</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.374300</td>\n      <td>0.368633</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.361300</td>\n      <td>0.366010</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.357100</td>\n      <td>0.361277</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.357500</td>\n      <td>0.360566</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4380, training_loss=0.5943609960547321, metrics={'train_runtime': 4982.773, 'train_samples_per_second': 3.516, 'train_steps_per_second': 0.879, 'total_flos': 7.610142874927104e+16, 'train_loss': 0.5943609960547321, 'epoch': 5.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#resume_from_checkpoint=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:59:21.514067Z","iopub.execute_input":"2025-02-15T18:59:21.514337Z","iopub.status.idle":"2025-02-15T18:59:21.518220Z","shell.execute_reply.started":"2025-02-15T18:59:21.514304Z","shell.execute_reply":"2025-02-15T18:59:21.517628Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink('/kaggle/working/Llama_3.2_3b_fine_tuned_lora_model/adapter_model.safetensors')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:22:40.217387Z","iopub.execute_input":"2025-02-15T19:22:40.217761Z","iopub.status.idle":"2025-02-15T19:22:40.223816Z","shell.execute_reply.started":"2025-02-15T19:22:40.217730Z","shell.execute_reply":"2025-02-15T19:22:40.223075Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/Llama_3.2_3b_fine_tuned_lora_model/adapter_model.safetensors","text/html":"<a href='/kaggle/working/Llama_3.2_3b_fine_tuned_lora_model/adapter_model.safetensors' target='_blank'>/kaggle/working/Llama_3.2_3b_fine_tuned_lora_model/adapter_model.safetensors</a><br>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from safetensors.torch import load_model, save_model\nimport torch\n\n# Assume `model` is your trained PyTorch model\nmodel_path = \"/kaggle/working/Llama_3.2_3b_fine_tuned_lora_model/adapter_model.safetensors\"\n\nsave_model(peft_model, model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:08:20.561985Z","iopub.execute_input":"2025-02-15T19:08:20.562295Z","iopub.status.idle":"2025-02-15T19:09:26.701201Z","shell.execute_reply.started":"2025-02-15T19:08:20.562268Z","shell.execute_reply":"2025-02-15T19:09:26.700355Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\n# Generate a correct download link\nfrom IPython.display import FileLink\n\nFileLink(\"/wandb/debug-internal.log\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T19:47:40.983386Z","iopub.execute_input":"2025-02-15T19:47:40.983745Z","iopub.status.idle":"2025-02-15T19:47:40.990202Z","shell.execute_reply.started":"2025-02-15T19:47:40.983717Z","shell.execute_reply":"2025-02-15T19:47:40.989326Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"/wandb/debug-internal.log","text/html":"Path (<tt>/wandb/debug-internal.log</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}