{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Load JSON file\nfile_path = \"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    data = json.load(file)\n\n# Extract relevant information\nrecords = []\nfor topic in data[\"data\"]:\n    title = topic[\"title\"]\n    for paragraph in topic[\"paragraphs\"]:\n        context = paragraph[\"context\"]\n        for qas in paragraph[\"qas\"]:\n            question = qas[\"question\"]\n            question_id = qas[\"id\"]\n            answers = [ans[\"text\"] for ans in qas[\"answers\"]]  # Collect all possible answers\n            records.append([title, context, question, question_id, answers])\n\n# Convert to Pandas DataFrame\ndf = pd.DataFrame(records, columns=[\"Title\", \"Context\", \"Question\", \"Question_ID\", \"Answers\"])\n\n# Display DataFrame\nprint(df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:00:22.745296Z","iopub.execute_input":"2025-02-17T13:00:22.745621Z","iopub.status.idle":"2025-02-17T13:00:25.044787Z","shell.execute_reply.started":"2025-02-17T13:00:22.745577Z","shell.execute_reply":"2025-02-17T13:00:25.043767Z"}},"outputs":[{"name":"stdout","text":"                      Title  \\\n0  University_of_Notre_Dame   \n1  University_of_Notre_Dame   \n2  University_of_Notre_Dame   \n3  University_of_Notre_Dame   \n4  University_of_Notre_Dame   \n\n                                             Context  \\\n0  Architecturally, the school has a Catholic cha...   \n1  Architecturally, the school has a Catholic cha...   \n2  Architecturally, the school has a Catholic cha...   \n3  Architecturally, the school has a Catholic cha...   \n4  Architecturally, the school has a Catholic cha...   \n\n                                            Question  \\\n0  To whom did the Virgin Mary allegedly appear i...   \n1  What is in front of the Notre Dame Main Building?   \n2  The Basilica of the Sacred heart at Notre Dame...   \n3                  What is the Grotto at Notre Dame?   \n4  What sits on top of the Main Building at Notre...   \n\n                Question_ID                                    Answers  \n0  5733be284776f41900661182               [Saint Bernadette Soubirous]  \n1  5733be284776f4190066117f                [a copper statue of Christ]  \n2  5733be284776f41900661180                        [the Main Building]  \n3  5733be284776f41900661181  [a Marian place of prayer and reflection]  \n4  5733be284776f4190066117e       [a golden statue of the Virgin Mary]  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhuggingface_value = user_secrets.get_secret(\"huggingface\")\nwandb_value = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:00:25.046169Z","iopub.execute_input":"2025-02-17T13:00:25.046660Z","iopub.status.idle":"2025-02-17T13:00:25.656382Z","shell.execute_reply.started":"2025-02-17T13:00:25.046624Z","shell.execute_reply":"2025-02-17T13:00:25.655494Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=huggingface_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:00:25.658039Z","iopub.execute_input":"2025-02-17T13:00:25.658342Z","iopub.status.idle":"2025-02-17T13:00:26.475467Z","shell.execute_reply.started":"2025-02-17T13:00:25.658310Z","shell.execute_reply":"2025-02-17T13:00:26.474652Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\nimport torch\n#TinyLlama/TinyLlama_v1.1\n#meta-llama/Llama-3.2-3B\n# Load the model and tokenizer\nmodel_name = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name,device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:00:26.476662Z","iopub.execute_input":"2025-02-17T13:00:26.476883Z","iopub.status.idle":"2025-02-17T13:00:56.407531Z","shell.execute_reply.started":"2025-02-17T13:00:26.476863Z","shell.execute_reply":"2025-02-17T13:00:56.406708Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e173884e8e244b6d9630013ea42a1202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b827543438a64d9d8a56724868e99a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab8450f763cd4082ad4dee9575e4208e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450111a81f424724a7c86354e7dcd59c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c72fee827c4ed3ba3fec1149821b32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"561e6d0061584c8ea17e05968c7d7e9a"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from datasets import Dataset , DatasetDict\nmax_length = 256\ndef tokenize_function(examples):\n    text = [f\"Context: {inp} \\nGenerated Question:\" for inp in examples[\"Context\"]]\n    \n    examples[\"input_ids\"] = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors = \"pt\").input_ids\n    examples[\"labels\"] = tokenizer(examples[\"Question\"], truncation=True, padding=\"max_length\", max_length=max_length, return_tensors = \"pt\").input_ids\n    \n    return examples\n    \ndata = Dataset.from_pandas(df)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenized_dataset = data.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns(['Title', 'Context', 'Question', 'Question_ID', 'Answers'])\n# tokenized_dataset = tokenized_dataset.filter(lambda example,index:index%20==0,with_indices=True)\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:00:56.408315Z","iopub.execute_input":"2025-02-17T13:00:56.408911Z","iopub.status.idle":"2025-02-17T13:01:40.621003Z","shell.execute_reply.started":"2025-02-17T13:00:56.408887Z","shell.execute_reply":"2025-02-17T13:01:40.620156Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8825c2cd4fdb4542a91a89c3d84465d7"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha = 32,\n    lora_dropout=0.05,\n    bias='none',\n    task_type = TaskType.SEQ_2_SEQ_LM\n)\npeft_model = get_peft_model(model,peft_config=lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:01:40.622039Z","iopub.execute_input":"2025-02-17T13:01:40.622361Z","iopub.status.idle":"2025-02-17T13:01:40.686912Z","shell.execute_reply.started":"2025-02-17T13:01:40.622329Z","shell.execute_reply":"2025-02-17T13:01:40.686102Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# from datasets import Dataset , DatasetDict\n# split_dataset = DatasetDict({\n#     \"train\": split_dataset[\"train\"].with_format(\"torch\"),  \n#     \"test\": split_dataset[\"test\"].with_format(\"torch\")\n# })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:01:40.687742Z","iopub.execute_input":"2025-02-17T13:01:40.688008Z","iopub.status.idle":"2025-02-17T13:01:40.691018Z","shell.execute_reply.started":"2025-02-17T13:01:40.687988Z","shell.execute_reply":"2025-02-17T13:01:40.690298Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments , Trainer\ntraining_args = TrainingArguments(\n                    output_dir=\" ./llama_3.2_3b_fine-tuned\",\n                    learning_rate = 1e-5,\n                    num_train_epochs = 5,\n                    weight_decay = 0.01,\n                    # per_device_train_batch_size=2,\n                    # per_device_eval_batch_size=2,\n                    # gradient_accumulation_steps=4,\n                    auto_find_batch_size = True,\n                    evaluation_strategy  = 'epoch',\n                    save_strategy=\"epoch\",  # Save checkpoints at each epoch\n                    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n                    load_best_model_at_end=True,  # Load best model after training\n                    fp16=True)\n\ntrainer = Trainer(\n    model = peft_model,\n    # tokenizer = tokenizer,\n    args = training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:01:40.693096Z","iopub.execute_input":"2025-02-17T13:01:40.693319Z","iopub.status.idle":"2025-02-17T13:01:42.585194Z","shell.execute_reply.started":"2025-02-17T13:01:40.693300Z","shell.execute_reply":"2025-02-17T13:01:42.584572Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=wandb_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:01:42.586146Z","iopub.execute_input":"2025-02-17T13:01:42.586363Z","iopub.status.idle":"2025-02-17T13:01:49.601608Z","shell.execute_reply.started":"2025-02-17T13:01:42.586343Z","shell.execute_reply":"2025-02-17T13:01:49.600880Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbahaa_beshoy\u001b[0m (\u001b[33mbahaa_beshoy-helwan-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T13:01:49.602441Z","iopub.execute_input":"2025-02-17T13:01:49.603215Z","iopub.status.idle":"2025-02-17T14:55:33.550301Z","shell.execute_reply.started":"2025-02-17T13:01:49.603178Z","shell.execute_reply":"2025-02-17T14:55:33.549678Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250217_130150-9r7z3z02</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/9r7z3z02' target=\"_blank\"> ./llama_3.2_3b_fine-tuned</a></strong> to <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface' target=\"_blank\">https://wandb.ai/bahaa_beshoy-helwan-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/9r7z3z02' target=\"_blank\">https://wandb.ai/bahaa_beshoy-helwan-university/huggingface/runs/9r7z3z02</a>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='43800' max='43800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [43800/43800 1:53:34, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.148900</td>\n      <td>0.134290</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.143100</td>\n      <td>0.129126</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.137800</td>\n      <td>0.127241</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.138200</td>\n      <td>0.126283</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.138400</td>\n      <td>0.126035</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=43800, training_loss=0.1503094338717526, metrics={'train_runtime': 6822.8482, 'train_samples_per_second': 51.356, 'train_steps_per_second': 6.42, 'total_flos': 2.434643974422528e+16, 'train_loss': 0.1503094338717526, 'epoch': 5.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#resume_from_checkpoint=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:55:33.551146Z","iopub.execute_input":"2025-02-17T14:55:33.551435Z","iopub.status.idle":"2025-02-17T14:55:33.555309Z","shell.execute_reply.started":"2025-02-17T14:55:33.551404Z","shell.execute_reply":"2025-02-17T14:55:33.554575Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"peft_model.save_pretrained(\"./t5-small-lora-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:55:33.556027Z","iopub.execute_input":"2025-02-17T14:55:33.556312Z","iopub.status.idle":"2025-02-17T14:55:34.007605Z","shell.execute_reply.started":"2025-02-17T14:55:33.556281Z","shell.execute_reply":"2025-02-17T14:55:34.007009Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n# peft_model = PeftModel.from_pretrained(base_model, \"./t5-small-lora-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:55:34.008353Z","iopub.execute_input":"2025-02-17T14:55:34.008557Z","iopub.status.idle":"2025-02-17T14:55:34.012281Z","shell.execute_reply.started":"2025-02-17T14:55:34.008539Z","shell.execute_reply":"2025-02-17T14:55:34.011413Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"sample_context = \"\"\"Rats are nocturnal, and out in the night the brown rat’s eyes are small and black and\nshiny; when a flashlight shines into them in the dark, the eyes of a rat light up like the\neyes of a deer. Though it forages* in darkness, the brown rat has poor eyesight. It makes\nup for this with, first of all, an excellent sense of smell. . . . They have an excellent sense\nof taste, detecting the most minute amounts of poison, down to one part per million. A\nbrown rat has strong feet, the two front paws each equipped with four clawlike nails, the\nrear paws even longer and stronger. It can run and climb with squirrel-like agility. It is\nan excellent swimmer, surviving in rivers and bays, in sewer streams and toilet bowls.\"\"\"\n\n# Tokenize the input context\ninput_text = f\"Context: {sample_context} \\nGenerated Question:\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n\n# Generate questions\noutputs = model.generate(input_ids, max_length=256, num_beams=5, early_stopping=True)\n\n# Decode the generated question\ngenerated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Question:\", generated_question)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:58:23.680128Z","iopub.execute_input":"2025-02-17T14:58:23.680421Z","iopub.status.idle":"2025-02-17T14:58:23.959851Z","shell.execute_reply.started":"2025-02-17T14:58:23.680399Z","shell.execute_reply":"2025-02-17T14:58:23.958814Z"}},"outputs":[{"name":"stdout","text":"Generated Question: What is a brown rat?\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}